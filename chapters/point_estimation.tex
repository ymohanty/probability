
\chapter{Point estimation\label{chap:pointEstimation}}

\section{Population and sample}

\begin{example}
\label{exa:isi2009samplepsb11}
A simple random sample of size $n=n_1+n_2$ is drawn without replacement from a finite population of size $N$. Further a simple random sample of size $n_1$ is drawn without replacement from the first sample. Let $\bar{y}$ and $\overline{y_1}$ be the respective sample means.
(a) Find $V\left(\overline{y_1}\right)$ and $V\left(\overline{y_2}\right)$, where $\overline{y_2}$ is the mean of the remaining $n_2$ units in the first sample.
(b) Show that $\operatorname{Cov}\left(\overline{y_1}, \overline{y_2}\right)=-S^2 / N$, where $S^2$ is the population variance.
\hl{TODO}
\end{example}

\section{Sufficiency and completeness}
\begin{example}
\label{exa:isi2007samplepsb9}Let $X_{1}$ and $X_{2}$ be i.i.d.
random variables from $\operatorname{Bernoulli}(\theta)$ distribution.
Verify if the statistic $X_{1}+2X_{2}$ is sufficient for $\theta$.\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2006samplepsb7}Let $Y$ be a random variable with probability
density function 
\[
f_{Y}(y\mid\theta)=\begin{cases}
\frac{1}{\theta}e^{-y/\theta} & \text{ if }y>0,\\
0 & \text{ otherwise }
\end{cases}
\]
 with $\theta>0$. Suppose that the conditional distribution of $X$
given $Y=y$ is $N\left(y,\sigma^{2}\right)$, with $\sigma^{2}>0$.
Both $\theta$ and $\sigma^{2}$ are unknown parameters. Let $\left(X_{1},Y_{1}\right),\ldots,\left(X_{n},Y_{n}\right)$
be a random sample from the joint distribution of $X$ and $Y$. Find
a nontrivial joint sufficient statistic for $\left(\theta,\sigma^{2}\right)$.
\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2007samplepsb10}Suppose $X$ takes three values 1,2
and 3 with 
\[
P(X=k)=\begin{cases}
(1-\theta)/2 & \text{ if }k=1\\
1/2 & \text{ if }k=2\\
\theta/2 & \text{ if }k=3
\end{cases}
\]
 where $0<\theta<1$. Suppose that the following random sample of
size 10 was drawn from the above distribution : 
\[
1,3,1,2,3,1,2,2,1,1
\]

Find the m.le. of $\theta$ based on the above sample. \hl{TODO}
\end{example}


\section{Unbiasedness, consistency, and efficiency}
\begin{example}
\label{exa:isi2008samplepsb11} Let $r$ be the number of successes
in $n$ Bernoulli trials with unknown probability $p$ of success.
Obtain the minimum variance unbiased estimator of $p-p^{2}$.
\hl{TODO}
\end{example}

\begin{example}
	\label{exa:isi2009samplepsb7}
	Let $X$ be a random variable taking values $-1,0,1,2,3$ such that
	$$
	P_\theta(X=x)= \begin{cases}2 \theta(1-\theta) & \text { if } \quad x=-1, \\ \theta^x(1-\theta)^{3-x} & \text { if } \quad x=0,1,2,3 .\end{cases}
	$$
	where $\theta \in(0,1)$. Show that $E_\theta(U(X))=0$ for all $\theta \in(0,1)$ if and only if $U(k)=a U^{\star}(k)$ for some $a \in \mathbb{R}$, where
	$$
	U^{\star}(k)=\left\{\begin{array}{ccc}
		0 & \text { if } \quad k=0,3 \\
		1 & \text { if } \quad k=-1 \\
		-2 & \text { if } \quad k=1,2
	\end{array}\right.
	$$
	\hl{TODO}
\end{example}


\section{Properties of extremum estimators}
\begin{example}
\label{exa:isi2004samplepsb6}Let $Y_{1},Y_{2},Y_{3}$ and $Y_{4}$
be four uncorrelated random variables with 
\[
\E\left(Y_{i}\right)=i\theta,\quad\Var\left(Y_{i}\right)=i^{2}\sigma^{2},\quad i=1,2,3,4,
\]
 where $\theta$ and$\sigma(>0)$ are unknown parameters. Find the
values of $c_{1},c_{2},c_{3}$ and$c_{4}$ for which $\sum_{i=1}^{4}c_{i}Y_{i}$
is unbiased for $\theta$ and has least variance.\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2004samplepsb8}Let $X_{1},X_{2},\ldots X_{n}$ be i.
i. d. with common density $f(x;\theta)$ given by 
\[
f(x;\theta)=\frac{1}{2\theta}\exp(-|x|/\theta),\quad-\infty<x<\infty,\quad\theta\in(0,\infty).
\]

In case of each of the statistics $S$ and $T$ defined below, decide
(a) if it is an unbiased estimator of $\theta,$ (b) if it is an MLE
for $\theta$ and (c) if it is sufficient for $\theta$. Give reasons.
\hl{TODO}
\[
S=\frac{1}{n}\sum_{i=1}^{n}X_{i},\quad T=\frac{1}{n}\sum_{i=1}^{n}\left|X_{i}\right|
\]
\end{example}

\begin{example}
\label{exa:isi2005samplepsb7}Suppose that $X_{1},\ldots,X_{n}$ is
a random sample of size $n\geq1$ from a Poisson distribution with
parameter $\lambda$. Find the minimum variance inbiased estimator
of $e^{-\lambda}$. \hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2005samplepsb8}Suppose $X_{1},\ldots,X_{n}$ constitute
a random sample from a population with density 
\[
f(x,\theta)=\frac{x}{\theta^{2}}\exp\left(-\frac{x^{2}}{2\theta^{2}}\right),x>0,\theta>0.
\]

Find the Cramer-Rao lower bound to the variance of an unbiased estimator
of $\theta^{2}$. \hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2005samplepsb9}Let $X_{1},X_{2},X_{3}$ be independent
random variables such that $X_{i}$ is uniformly distributed in $(0,i\theta)$
for $i=1,2,3$. Find the maximum likelihood estimator of $\theta$
and examine whether it is unbiased for $\theta$. \hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2006samplepsb8}Let $\left(X_{1},Y_{1}\right),\ldots,\left(X_{n},Y_{n}\right)$
be a random sample from the discrete distribution with joint probability
mass function 
\[
f_{X,Y}(x,y)=\begin{cases}
\frac{\theta}{4} & (x,y)=(0,0)\text{ and }(1,1),\\
\frac{2-\theta}{4} & (x,y)=(0,1)\text{ and }(1,0),
\end{cases}
\]
 with $0\leq\theta\leq2$. Find the maximum likelihood estimator of
$\theta$. \hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2006samplepsb9}Let $X_{1},X_{2},\ldots$ be i.i.d.
random variables with density $f_{\theta}(x),x\in\mathbb{R},\theta\in(0,1)$
being the unknown parameter. Suppose that there exists an unbiased
estimator $T$ of $\theta$ based on sample size 1, i. e. $\E_{\theta}\left(T\left(X_{1}\right)\right)=\theta$.
Assume that $\Var\left(T\left(X_{1}\right)\right)<\infty$. (a) Find
an estimator $V_{n}$ for $\theta$ based on$X_{1},\ldots X_{n}$
such that $V_{n}$ is consistent for $\theta$. (b) Let $S_{n}$ be
the MVUE (minimum variance unbiased estimator) of $\theta$ based
on $X_{1},\ldots,X_{n}$. Show that $\lim_{n\rightarrow\infty}\Var\left(S_{n}\right)=0$.\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2008samplepsb9}Let $X_{1},\ldots,X_{m}$ be a random
sample from a uniform distribution on $\{1,2,\ldots,N\}$ where $N$
is an unknown positive integer. Find the MLE $\widehat{N}$ of $N$
and find its distribution function.\hl{TODO}
\end{example}

\begin{example}
	\label{exa:isi2009samplepsb6}
	Suppose $X_1, \ldots, X_n$ are i.i.d. $\mathrm{N}(\theta, 1), \theta_0 \leq \theta \leq \theta_1$, where $\theta_0<\theta_1$ are two specified numbers. Find the MLE of $\theta$ and show that it is better than the sample mean $\bar{X}$ in the sense of having smaller mean squared error.
	\hl{TODO}
\end{example}

\begin{example}
	\label{exa:isi2023psb5}
	Suppose $X_1, \ldots, X_n(n \geq 2)$ are independent and identically distributed observations from a distribution having probability density function
	
	$$
	f_\theta(x)= \begin{cases}e^{-(x-\theta)} & \text { if } x \geq \theta \\ 0 & \text { if } x<\theta\end{cases}
	$$
	
	where $\theta \in \mathbb{R}$. Let
	
	$$
	\psi(\theta)=\int_1^{\infty} f_\theta(x) d x
	$$
	
	
	Define $\widehat{\theta}_n=\min \left\{X_1, \ldots, X_n\right\}$. Consider $\psi\left(\widehat{\theta}_n\right)$ as an estimator of $\psi(\theta)$ and let $B_n(\theta)$ denote the associated bias.
	(a) Show that $B_n(\theta)>0$ for every $\theta<1$ and $B_n(\theta)=0$ for every $\theta \geq 1$.
	(b) Show that $\lim _{n \rightarrow \infty} B_n(\theta)=0$ for every $\theta<1$.
\end{example}

\begin{example}
	\label{exa:isi2023psb6}
	Suppose that two observations $X_1$ and $X_2$ are drawn at random from a distribution with the following probability density function
	
	$$
	f_\theta(x)= \begin{cases}\frac{1}{2 \theta} & \text { if } 0 \leq x \leq \theta \text { or } 2 \theta \leq x \leq 3 \theta \\ 0 & \text { otherwise }\end{cases}
	$$
	
	where $\theta>0$. Determine the maximum likelihood estimator of $\theta$ for each of the following observed values of $X_1$ and $X_2$.
	(a) $X_1=7$ and $X_2=9$.
	(b) $X_1=4$ and $X_2=9$
	(c) $X_1=5$ and $X_2=9$.
\end{example}


