
\chapter{\label{chap:finiteVectorSpace}Finite Dimensional Vector Spaces}
\begin{defn}
\label{def:vectorSpace}A \emph{vector space }is a non-empty set \emph{$V$
}over a field $\mathbb{K}$ together with two binary operations $+:V\times V\to V$
and $\cdot:\mathbb{K}\times V\to V$ , called \emph{vector addition
}and \emph{scalar multiplication, }respectively, that satisfy
\end{defn}

\begin{enumerate}
\item Closure under vector addition and scalar multiplication: For any $\alpha\in K$
and any $u,v\in V$: $\alpha u\in V$ and $u+v\in V.$
\item Associativity of vector addition: For any $u,v,w\in V:u+\left(v+w\right)=\left(v+u\right)+w$
\item Commutativity of vector addition: For any $u,v\in V:u+v=v+u$
\item Identity element of vector addition: There exists some element $\mathbf{0}\in V$,
called the \emph{zero vector }such that for any $v\in V:\mathbf{0}+v=v.$
\item Inverse element of vector addition: For any $v\in V$, there exists
a vector $-v\in V$ such that $v+\left(-v\right)=\mathbf{0}$
\item Compatibility of scalar multiplication with field multiplication:
For any $\alpha,\beta\in\mathbb{K}$ and $u\in V:\left(\alpha\beta\right)\cdot u=\alpha\cdot\left(\beta\cdot u\right).$
\item Identity element of scalar multiplication: There exists some element
$1\in\mathbb{K}$ such that for any $u\in V:1v=v.$
\item Distributivity of scalar multiplication with respect to vector addition:
For any $\alpha\in\mathbb{K}$ and any $u,v\in V:\alpha\cdot\left(u+v\right)=\alpha\cdot u+\alpha\cdot v$
\item Distributivity of scalar multiplication with respect to field addition:
For all $\alpha,\beta\in\mathbb{K}$ and $u\in V:\left(\alpha+\beta\right)\cdot u=\alpha\cdot u+\beta\cdot u.$
\end{enumerate}
\begin{xca}[ISI 2013 PSB-1]
\label{exer:isi2013psb1}Let $E=\{1,2,\ldots,n\}$, where \$n\$ is
an odd positive integer. Let $V$ be the vector space of all functions
from $E$ to $\mathbb{R}^{3}$, where the vector space operations
are given by 
\[
\begin{aligned}(f+g)(k) & =f(k)+g(k),\quad\text{ for }f,g\in V,k\in E,\\
(\lambda f)(k) & =\lambda f(k),\quad\text{ for }f\in V,\lambda\in\mathbb{R},k\in E.
\end{aligned}
\]
\end{xca}

\begin{enumerate}
\item Find the dimension of V
\item Let $T:V\to V$ be the map given by 
\[
T\left(f\right)\left(k\right):=\frac{1}{2}\left(f\left(k\right)+f\left(n+1-k\right)\right),\ k\in E
\]
is linear.
\item Find the null space of T.
\end{enumerate}

\section{Determinants\label{sec:Determinants}}

We shall construct the determinant of linear map $T:\R^{n}\to\R^{n}$axiomatically.
Determinants are typically defined by a given matrix representation
$M_{T}=\left[\begin{array}{cccc}
| & | &  & |\\
v_{1} & v_{2} & \ldots & v_{n}\\
| & | &  & |
\end{array}\right]$ of $T$ with respect to the standard basis on $\R^{n}$. Here $v_{i}$
are vectors in $\R^{n}$ with respect to the standard basis. As we
shall later see, the choice of basis does not matter and so we can
think of the determinant as acting on the linear map itself rather
the matrix representation.

\subsection{Desiderata of the determinant}

What properties should our determinant function have? Why do we even
need such a function? What does it actually represent? Alas, the determinant
is one of those concepts whose usefulness only becomes apparent \emph{after
}you are done constructing it and relaying its properties. In fact,
the geometric interpretation of the determinant is often not even
covered in a course on Linear Algebra; we shall in fact use the $n-$dimensional
Lebesgue measure in Section \ref{subsec:lebesgueRNproperties}to understand
the geometry of determinants in the main text rather than this appendix.

Thus. without much motivation (for now), we set out on a goal to construct
a map $\det:M_{n\times n}\to\R$ (where $M_{n\times n}$ is the vector
space of all $n\times n$ real valued matrices) that satisfies the
following properties:
\begin{enumerate}
\item \textbf{Linearity in each argument: }Let $\left\{ v_{i}\right\} _{i=1}^{n},u\in\R^{n}$\textbf{
}and let $\alpha\in\R$. Then\textbf{
\[
\det\left(\left[\begin{array}{ccccc}
| &  & | &  & |\\
v_{1} & \ldots & v_{k}+\alpha u & \ldots & v_{n}\\
| &  & | &  & |
\end{array}\right]\right)=\det\left(\left[\begin{array}{ccccc}
| &  & | &  & |\\
v_{1} & \ldots & v_{k} & \ldots & v_{n}\\
| &  & | &  & |
\end{array}\right]\right)+\alpha\det\left(\left[\begin{array}{ccccc}
| &  & | &  & |\\
v_{1} & \ldots & u & \ldots & v_{n}\\
| &  & | &  & |
\end{array}\right]\right).
\]
}
\item \textbf{Preservation under column replacement:} Let $\left\{ v_{i}\right\} _{i=1}^{n}\in\R^{n}$\textbf{
}and let $\alpha\in\R$. Then
\[
\det\left(\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{l}+\alpha v_{k} & \ldots & v_{k} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]\right)=\det\left(\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{l} & \ldots & v_{k} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]\right).
\]
\item \textbf{Antisymmetry: }Let $\left\{ v_{i}\right\} _{i=1}^{n}\in\R^{n}$.
Then
\[
\det\left(\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{l} & \ldots & v_{k} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]\right)=-\det\left(\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{k} & \ldots & v_{l} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]\right).
\]
\item \textbf{Normalization: $\det\left(I_{n}\right)=1$ }where $I_{n}$
is the $n\times n$ identity matrix.
\end{enumerate}
It turns out that these properties completely characterize the determinant,
in that there exists a unique function that possesses these properties.
We shall postpone the proof of existence and uniqueness to the end
of this section and derive the properties of the determinant function.

\subsection{Properties of the determinant}

All the properties of the determinant can be recovered using the axioms
above, although some results are harder to prove than others. We start
with a few simple results.
\begin{prop}
\label{prop:IzeroColumnDetZero}If $A\in M_{n\times n}$ has a zero
column, then $\det\left(A\right)=0$.
\end{prop}

\begin{proof}
This fact follows from linearity since a zero column can be written
as $0.v$ for any $v\in\R^{n}$.
\end{proof}
\begin{prop}
\label{prop:linearlyDependentColumnsDetZero}If $A\in M_{n\times n}$
has linearly dependent columns then $\det\left(A\right)=0$.
\end{prop}

\begin{proof}
Let 
\[
A=\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{l} & \ldots & v_{k} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]
\]
and notice that if the columns of $A$ are linearly dependent then
there exist constants $\left\{ \alpha_{i}\right\} _{i=1}^{n}\in\R$,,
such that $\sum_{i=1i\neq l}^{n}\alpha_{i}v_{i}=v_{l}.$ Now, we can
write
\begin{align*}
\det\left(A\right) & =\det\left(\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & \sum_{i=1i\neq l}^{n}\alpha_{i}v_{i} & \ldots & v_{k} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]\right)\\
 & =\sum_{i\neq l}\alpha_{i}\det\left(\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{i} & \ldots & v_{k} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]\right)\\
 & =0
\end{align*}
where in the second equality we have used linearity and in the last
equality we have used anti-symmetry and the fact that if $\det\left(B\right)=-\det\left(B\right)$
then $\det\left(B\right)=0.$
\end{proof}
Now we have sufficiently many properties to describe the determinants
of the most elementary types of matrices. Recall that the elementary
linear maps $T:\R^{n}\to\R^{n}$ are
\begin{enumerate}
\item \textbf{Row scaling: $T\left(x_{1,}\ldots,x_{k},\ldots,x_{n}\right)^{T}=\left(x_{1},\ldots cx_{k},\ldots x_{n}\right)^{T}$
}for some $c\in\R$. The matrix with respect to the standard basis
of this transformation is $\diag\left(1,1,\ldots,c,\ldots,1\right).$
By linearity and the normalization property of determinants, $\det\left(T\right)=c.$
More generally, for any diagonal matrix $D=\diag\left(a_{1},a_{2},\ldots,d_{n}\right)$
we have that $\det\left(D\right)=\prod_{i=1}^{n}a_{i}$ by linearity
and normalization. $T$ is invertible with $T^{-1}$ being represented
by $\diag\left(1,1,\ldots,\frac{1}{c},\ldots,1\right)$. The tranpose
of $T^{T}=T$
\item \textbf{Row switching: $T\left(x_{1},\ldots x_{l},\ldots x_{k},\ldots,x_{n}\right)^{T}=\left(x_{1},\ldots,x_{k},\ldots,x_{l},\ldots,x_{n}\right)^{T}$
}for any $1\leq l\leq k\leq n.$ The matrix of this operator wrt to
the standard basis is $\left[e_{1},\ldots e_{k},\ldots,e_{l},\ldots,e_{n}\right]$
where $e_{i}$ is the $i$th standard basis element. Clearly, $\det\left(T\right)=-1$
by antisymmetry. Again, $T$ is invertible and is its own inverse.
The tranpose of $T$ is $T$ itself.
\item \textbf{Row replacement$T\left(x_{1},\ldots x_{l},\ldots x_{k},\ldots,x_{n}\right)^{T}=\left(x_{1},\ldots,x_{l}+cx_{k},\ldots,x_{k},\ldots,x_{n}\right)^{T}$}for
any $1\leq l<l\leq n$ and any $c\in\R$. The matrix representation
of this map is given by $\left[e_{1},\ldots,e_{l},\ldots,e_{k}+ce_{l},\ldots,e_{n}\right]$
with determinant $\det\left(T\right)=1$ by linearity and Proposition
\ref{prop:linearlyDependentColumnsDetZero}. This function is also
clearly invertible and its inverse is an operations of the same type.
The tranpose is also an operation of the same type.
\end{enumerate}
\begin{prop}
\label{prop:detMatrixTimesElementary}Let $A\in M_{n\times n}$ and
let $T\in M_{n\times n}$ be the standard basis representation of
an elementary row operation as above. Then
\[
\det\left(AT\right)=\det\left(A\right)\det\left(T\right).
\]
\end{prop}

\begin{proof}
First consider the case when $T$ is the scaling operator for the
$k$th coordinate. Then, letting $A=\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{l} & \ldots & v_{k} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]$, we have $ST=\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{l} & \ldots & cv_{k} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]$ and so 
\[
\det\left(AT\right)=c\det\left(A\right)=\det\left(A\right)\det\left(T\right).
\]
Next, let $T$ is the row switching operator. Then $AT=\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{k} & \ldots & v_{l} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]$ and by antisymmetry
\[
\det\left(AT\right)=-\det\left(A\right)=\det\left(A\right)\det\left(T\right).
\]
Finally, if $T$ is the row replacement operator, then $AT=\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{l} & \ldots & v_{k}+cv_{l} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]$ and 
\[
\det\left(AT\right)=\det\left(A\right)+c\det\left(\left[\begin{array}{ccccccc}
| &  & | &  & | &  & |\\
v_{1} & \ldots & v_{l} & \ldots & v_{l} & \ldots & v_{n}\\
| &  & | &  & | &  & |
\end{array}\right]\right)=\det\left(A\right)
\]
 by linear dependence.
\end{proof}
Note that by our work on simultaneous linear equations and their solutions,
every invertible matrix $A\in M_{n\times n}$ can be reduced to the
identity matrix by multiplication with elementary matrices from the
left i.e. $I=T_{1}T_{2}..T_{k}A.$ Then,
\[
A=T_{k}^{-1}T_{k-1}^{-1}\ldots T_{1}^{-1}
\]
where each $T_{i}^{-1}$ is an elementary operation and so $\det\left(A\right)=\prod_{k=1}^{n}\det\left(T_{i}^{-1}\right)$
by inducting on Proposition \ref{prop:detMatrixTimesElementary}.
\begin{prop}
\label{prop:detProduct}Let $A,B\in M_{n\times n}$. Then, 
\[
\det\left(AB\right)=\det\left(BA\right)=\det\left(A\right)\det\left(B\right).
\]
\end{prop}

\begin{proof}
First suppose that $A,B$ are both invertible. Then $A$ and $B$
are products of elementary transformations and so $\det\left(AB\right)=\det\left(A\right)\det\left(B\right).$
If either, $A$ or $B$ is not invertible, then neither $AB$ nor
$BA$ are invertible since neither operator is either injective or
surjective (by rank-nullity) but have the same domains. Suppose without
loss of generality that $B$ is not invertible. Then by the rank nullity
theorem, $\ker\left(B\right)$ is trivial, which is equivalent to
the fact $B$ has linearly dependent columns and so $\det\left(B\right)=0$.
The same argument applied to $AB$ or $BA$ shows that $\det\left(AB\right)=\det\left(BA\right)=0.$
This completes the proof.
\end{proof}
\begin{cor}
\label{cor:invertibleDetZero}A matrix $A\in M_{n\times n}$ is invertible
if and only if $\det\left(A\right)\neq0.$
\end{cor}

\begin{proof}
We have already shown that if $A$ is not invertible, then $A$ has
linearly dependent columns and so $\det\left(A\right)=0.$ Conversely,
suppose that $\det\left(A\right)=0$ and $A$ is invertible. Then
$1=\det\left(I\right)=\det\left(AA^{-1}\right)=\det\left(A\right)\det\left(A^{-1}\right)=0$
which is a contradiction.
\end{proof}
\begin{cor}
\label{cor:detOfInverse}Let $A\in M_{n\times n}$ be invertible.
Then
\[
\det\left(A^{-1}\right)=\frac{1}{\det\left(A\right)}.
\]
\end{cor}

\begin{proof}
Note that $1=\det\left(I\right)=\det\left(AA^{-1}\right)=\det\left(A\right)\det\left(A^{-1}\right).$
\end{proof}
%
\begin{cor}
\label{cor:detBasisInvariance}If $A,B\in M_{n\times n}$ are similar
matrices, then 
\[
\det\left(A\right)=\det\left(B\right).
\]
\end{cor}

\begin{proof}
Let $P\in M_{n\times n}$ be invertible such that 
\[
A=PBP^{-1}.
\]
Then, 
\begin{align*}
\det\left(A\right) & =\det\left(PBP^{-1}\right)\\
 & =\det\left(P\right)\det\left(B\right)\det\left(P^{-1}\right)\\
 & =\det\left(P\right)\det\left(B\right)\frac{1}{\det\left(P\right)}\\
 & =\det\left(B\right).
\end{align*}
\end{proof}
Note that this tells us that the determinant of a matrix doesn't change
under a change of basis. In other words, we can think of the determinant
as acting on the operator that the matrix represents rather than the
matrix itself.
\begin{prop}
\label{prop:detTranspose}For any $A\in M_{n\times n}$
\[
\det\left(A\right)=\det\left(A^{T}\right).
\]
\end{prop}

\begin{proof}
If $A$ is not invertible then $A^{T}$ is also not invertible and
so both determinants are zero. On the other hand, if $A$ is invertible
then $A=T_{1}T_{2}\ldots T_{k}$ for some $k$ where $T_{i}$ are
elementary matrices. Then, $A^{T}=T_{k}^{T}T_{k-1}^{T}\ldots T_{1}^{T}$,
where the determinant of $T_{i}$ is the same as the determinant of
$T_{i}^{T}$. The result then follows by Proposition \ref{prop:detProduct}.
\end{proof}

\subsection{Existence and uniqueness}

\subsection{The cofactor expansion}
\begin{example}[ISI 2023 PSB 1]
\label{exa:isi2023psb1}Let $A_{n}=\left(\left(a_{ij}\right)\right)$
be the $n\times n$ matrix defined by 
\[
a_{ij}=\begin{cases}
0 & \text{ if }|i-j|>1\\
1 & \text{ if }|i-j|=1\\
2 & \text{ if }i=j
\end{cases}
\]
What is the determinant of $A_{n}$ for $n\geq1$?\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2004psbsample2}Is the following system of equations
always consistent for real $k$ ? Justify your answer. 
\[
\begin{aligned}x+y+kz & =2,\\
3x+4y+2z & =k,\\
2x+3y-z & =1.
\end{aligned}
\]

Find the value of $k$ for which this system admits more than one
solution? Express the general solution for the system of equations
for this value of $k$.\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2005samplepsb1}Let $A$ be a $n\times n$ upper triangular
matrix such that $AA^{T}=A^{T}A$. Show that $A$ is a diagonal matrix.\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2005samplepsb3}Let $A$ be a $n\times n$ orthogonal
matrix, where $n$ is even and suppose \$|A|=\$ -1, where $|A|$ denotes
the determinant of $A$. Show that $|I-A|=0$, where $I$ denotes
the $n\times n$ identity matrix.\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2006samplepsb1}Let $A$ and $B$ be two invertible
$n\times n$ real matrices. Assume that $A+B$ is invertible. Show
that $A^{-1}+B^{-1}$ is also invertible.\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2007samplepsb1}Let $A$ be a $2\times2$ matrix with
real entries such that$A^{2}=0$. Find the determinant of $I+A$ where
$I$ denotes the identity matrix.\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2007samplepsb2}Let $A$ and $B$ be $n\times n$ real
matrices such that $A^{2}=A$ and $B^{2}=B$. Suppose that $I-(A+B)$
is invertible. Show that $\operatorname{rank}(A)=\operatorname{rank}(B)$.\hl{TODO}
\end{example}

\begin{example}
\label{exa:isi2008samplepsb1}1. Let 
\[
A=\frac{1}{3}\left(\begin{array}{rrr}
2 & -1 & -1\\
-1 & 2 & -1\\
-1 & -1 & 2
\end{array}\right).
\]

Which of the following statements are false. In each case, justify
your answer. (a) $A$ has only one real eigenvalue. (b) $\operatorname{Rank}(A)=\operatorname{Trace}(A)$.
(c) Determinant of $A$ equals the determinant of $A^{n}$ for each
integer $n>1$.\hl{TODO}
\end{example}


\section{Eigenvalues and eigenvectors\label{sec:finiteDimEigenvalues}}
